{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNu+kz2mVTMNER2s4b+eNIg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gtindi/Player_contact_detection/blob/main/contactDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyp6wHKHeZta"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "class Config:\n",
        "    AUTHOR = \"colum2131\"\n",
        "\n",
        "    NAME = \"NFLC-\" + \"Exp001-simple-xgb-baseline\"\n",
        "\n",
        "    COMPETITION = \"nfl-player-contact-detection\"\n",
        "\n",
        "    seed = 42\n",
        "    num_fold = 5\n",
        "    \n",
        "    xgb_params = {\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'auc',\n",
        "        'learning_rate':0.03,\n",
        "        'tree_method':'hist' if not torch.cuda.is_available() else 'gpu_hist'\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import subprocess\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from IPython.display import Video, display\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "import cv2\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    matthews_corrcoef,\n",
        ")\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    import cupy \n",
        "    import cudf\n",
        "    from cuml import ForestInference\n"
      ],
      "metadata": {
        "id": "J55Pgf09e7__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup(cfg):\n",
        "    cfg.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    # set dirs\n",
        "    cfg.INPUT = f'../input/{cfg.COMPETITION}'\n",
        "    cfg.EXP = cfg.NAME\n",
        "    cfg.OUTPUT_EXP = cfg.NAME\n",
        "    cfg.SUBMISSION = './'\n",
        "    cfg.DATASET = '../input/'\n",
        "\n",
        "    cfg.EXP_MODEL = os.path.join(cfg.EXP, 'model')\n",
        "    cfg.EXP_FIG = os.path.join(cfg.EXP, 'fig')\n",
        "    cfg.EXP_PREDS = os.path.join(cfg.EXP, 'preds')\n",
        "\n",
        "    # make dirs\n",
        "    for d in [cfg.EXP_MODEL, cfg.EXP_FIG, cfg.EXP_PREDS]:\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "        \n",
        "    return cfg"
      ],
      "metadata": {
        "id": "LiANV-ZzzgiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# function\n",
        "# ==============================\n",
        "# ref: https://www.kaggle.com/code/robikscube/nfl-player-contact-detection-getting-started\n",
        "def add_contact_id(df):\n",
        "    # Create contact ids\n",
        "    df[\"contact_id\"] = (\n",
        "        df[\"game_play\"]\n",
        "        + \"_\"\n",
        "        + df[\"step\"].astype(\"str\")\n",
        "        + \"_\"\n",
        "        + df[\"nfl_player_id_1\"].astype(\"str\")\n",
        "        + \"_\"\n",
        "        + df[\"nfl_player_id_2\"].astype(\"str\")\n",
        "    )\n",
        "    return df\n",
        "\n",
        "def expand_contact_id(df):\n",
        "    \"\"\"\n",
        "    Splits out contact_id into seperate columns.\n",
        "    \"\"\"\n",
        "    df[\"game_play\"] = df[\"contact_id\"].str[:12]\n",
        "    df[\"step\"] = df[\"contact_id\"].str.split(\"_\").str[-3].astype(\"int\")\n",
        "    df[\"nfl_player_id_1\"] = df[\"contact_id\"].str.split(\"_\").str[-2]\n",
        "    df[\"nfl_player_id_2\"] = df[\"contact_id\"].str.split(\"_\").str[-1]\n",
        "    return df\n",
        "\n",
        "# cross validation\n",
        "def get_groupkfold(train, target_col, group_col, n_splits):\n",
        "    kf = GroupKFold(n_splits=n_splits)\n",
        "    generator = kf.split(train, train[target_col], train[group_col])\n",
        "    fold_series = []\n",
        "    for fold, (idx_train, idx_valid) in enumerate(generator):\n",
        "        fold_series.append(pd.Series(fold, index=idx_valid))\n",
        "    fold_series = pd.concat(fold_series).sort_index()\n",
        "    return fold_series\n",
        "\n",
        "# xgboost code\n",
        "def fit_xgboost(cfg, X, y, params, add_suffix=''):\n",
        "    \"\"\"\n",
        "    xgb_params = {\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'auc',\n",
        "        'learning_rate':0.01,\n",
        "        'tree_method':'gpu_hist'\n",
        "    }\n",
        "    \"\"\"\n",
        "    oof_pred = np.zeros(len(y), dtype=np.float32)\n",
        "    for fold in sorted(cfg.folds.unique()):\n",
        "        if fold == -1: continue\n",
        "        idx_train = (cfg.folds!=fold)\n",
        "        idx_valid = (cfg.folds==fold)\n",
        "        x_train, y_train = X[idx_train], y[idx_train]\n",
        "        x_valid, y_valid = X[idx_valid], y[idx_valid]\n",
        "        display(pd.Series(y_valid).value_counts())\n",
        "\n",
        "        xgb_train = xgb.DMatrix(x_train, label=y_train)\n",
        "        xgb_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
        "        evals = [(xgb_train,'train'),(xgb_valid,'eval')]\n",
        "\n",
        "        model = xgb.train(\n",
        "            params,\n",
        "            xgb_train,\n",
        "            num_boost_round=10_000,\n",
        "            early_stopping_rounds=100,\n",
        "            evals=evals,\n",
        "            verbose_eval=100,\n",
        "        )\n",
        "\n",
        "        model_path = os.path.join(cfg.EXP_MODEL, f'xgb_fold{fold}{add_suffix}.model')\n",
        "        model.save_model(model_path)\n",
        "        if not torch.cuda.is_available():\n",
        "            model = xgb.Booster().load_model(model_path)\n",
        "        else:\n",
        "            model = ForestInference.load(model_path, output_class=True, model_type='xgboost')\n",
        "        pred_i = model.predict_proba(x_valid)[:, 1]\n",
        "        oof_pred[x_valid.index] = pred_i\n",
        "        score = round(roc_auc_score(y_valid, pred_i), 5)\n",
        "        print(f'Performance of the prediction: {score}\\n')\n",
        "        del model; gc.collect()\n",
        "\n",
        "    np.save(os.path.join(cfg.EXP_PREDS, f'oof_pred{add_suffix}'), oof_pred)\n",
        "    score = round(roc_auc_score(y, oof_pred), 5)\n",
        "    print(f'All Performance of the prediction: {score}')\n",
        "    return oof_pred\n",
        "\n",
        "def pred_xgboost(X, data_dir, add_suffix=''):\n",
        "    models = glob(os.path.join(data_dir, f'xgb_fold*{add_suffix}.model'))\n",
        "    if not torch.cuda.is_available():\n",
        "         models = [xgb.Booster().load_model(model_path) for model in models]\n",
        "    else:\n",
        "        models = [ForestInference.load(model, output_class=True, model_type='xgboost') for model in models]\n",
        "    preds = np.array([model.predict_proba(X)[:, 1] for model in models])\n",
        "    preds = np.mean(preds, axis=0)\n",
        "    return preds"
      ],
      "metadata": {
        "id": "C1Xho-WrznCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# read data\n",
        "# ==============================\n",
        "cfg = setup(Config)\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    tr_tracking = pd.read_csv(os.path.join(cfg.INPUT, 'train_player_tracking.csv'), parse_dates=[\"datetime\"])\n",
        "    te_tracking = pd.read_csv(os.path.join(cfg.INPUT, 'test_player_tracking.csv'), parse_dates=[\"datetime\"])\n",
        "    # tr_helmets = pd.read_csv(os.path.join(cfg.INPUT, 'train_baseline_helmets.csv'))\n",
        "    # te_helmets = pd.read_csv(os.path.join(cfg.INPUT, 'test_baseline_helmets.csv'))\n",
        "    # tr_video_metadata = pd.read_csv(os.path.join(cfg.INPUT, 'train_video_metadata.csv'))\n",
        "    # te_video_metadata = pd.read_csv(os.path.join(cfg.INPUT, 'test_video_metadata.csv'))\n",
        "    sub = pd.read_csv(os.path.join(cfg.INPUT, 'sample_submission.csv'))\n",
        "\n",
        "    train = pd.read_csv(os.path.join(cfg.INPUT, 'train_labels.csv'), parse_dates=[\"datetime\"])\n",
        "    test = expand_contact_id(sub)\n",
        "    \n",
        "else:\n",
        "    tr_tracking = cudf.read_csv(os.path.join(cfg.INPUT, 'train_player_tracking.csv'), parse_dates=[\"datetime\"])\n",
        "    te_tracking = cudf.read_csv(os.path.join(cfg.INPUT, 'test_player_tracking.csv'), parse_dates=[\"datetime\"])\n",
        "    # tr_helmets = cudf.read_csv(os.path.join(cfg.INPUT, 'train_baseline_helmets.csv'))\n",
        "    # te_helmets = cudf.read_csv(os.path.join(cfg.INPUT, 'test_baseline_helmets.csv'))\n",
        "    # tr_video_metadata = cudf.read_csv(os.path.join(cfg.INPUT, 'train_video_metadata.csv'))\n",
        "    # te_video_metadata = cudf.read_csv(os.path.join(cfg.INPUT, 'test_video_metadata.csv'))\n",
        "    sub = pd.read_csv(os.path.join(cfg.INPUT, 'sample_submission.csv'))\n",
        "\n",
        "    train = cudf.read_csv(os.path.join(cfg.INPUT, 'train_labels.csv'), parse_dates=[\"datetime\"])\n",
        "    test = cudf.DataFrame(expand_contact_id(sub))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "QM74sUGKzuGh",
        "outputId": "7ee29c27-71b3-46f6-a24e-31b9f07ca993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-67dc9c201fb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINPUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_player_tracking.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"datetime\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mte_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINPUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_player_tracking.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"datetime\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# tr_helmets = pd.read_csv(os.path.join(cfg.INPUT, 'train_baseline_helmets.csv'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/nfl-player-contact-detection/train_player_tracking.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# feature engineering\n",
        "# ==============================\n",
        "def create_features(df, tr_tracking, merge_col=\"step\", use_cols=[\"x_position\", \"y_position\"]):\n",
        "    output_cols = []\n",
        "    df_combo = (\n",
        "        df.astype({\"nfl_player_id_1\": \"str\"})\n",
        "        .merge(\n",
        "            tr_tracking.astype({\"nfl_player_id\": \"str\"})[\n",
        "                [\"game_play\", merge_col, \"nfl_player_id\",] + use_cols\n",
        "            ],\n",
        "            left_on=[\"game_play\", merge_col, \"nfl_player_id_1\"],\n",
        "            right_on=[\"game_play\", merge_col, \"nfl_player_id\"],\n",
        "            how=\"left\",\n",
        "        )\n",
        "        .rename(columns={c: c+\"_1\" for c in use_cols})\n",
        "        .drop(\"nfl_player_id\", axis=1)\n",
        "        .merge(\n",
        "            tr_tracking.astype({\"nfl_player_id\": \"str\"})[\n",
        "                [\"game_play\", merge_col, \"nfl_player_id\"] + use_cols\n",
        "            ],\n",
        "            left_on=[\"game_play\", merge_col, \"nfl_player_id_2\"],\n",
        "            right_on=[\"game_play\", merge_col, \"nfl_player_id\"],\n",
        "            how=\"left\",\n",
        "        )\n",
        "        .drop(\"nfl_player_id\", axis=1)\n",
        "        .rename(columns={c: c+\"_2\" for c in use_cols})\n",
        "        .sort_values([\"game_play\", merge_col, \"nfl_player_id_1\", \"nfl_player_id_2\"])\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    output_cols += [c+\"_1\" for c in use_cols]\n",
        "    output_cols += [c+\"_2\" for c in use_cols]\n",
        "    \n",
        "    if (\"x_position\" in use_cols) & (\"y_position\" in use_cols):\n",
        "        index = df_combo['x_position_2'].notnull()\n",
        "        if torch.cuda.is_available():\n",
        "            index = index.to_array()\n",
        "        distance_arr = np.full(len(index), np.nan)\n",
        "        tmp_distance_arr = np.sqrt(\n",
        "            np.square(df_combo.loc[index, \"x_position_1\"] - df_combo.loc[index, \"x_position_2\"])\n",
        "            + np.square(df_combo.loc[index, \"y_position_1\"]- df_combo.loc[index, \"y_position_2\"])\n",
        "        )\n",
        "        if torch.cuda.is_available():\n",
        "            tmp_distance_arr = tmp_distance_arr.to_array()\n",
        "        distance_arr[index] = tmp_distance_arr\n",
        "        df_combo['distance'] = distance_arr\n",
        "        output_cols += [\"distance\"]\n",
        "        \n",
        "    df_combo['G_flug'] = (df_combo['nfl_player_id_2']==\"G\")\n",
        "    output_cols += [\"G_flug\"]\n",
        "    return df_combo, output_cols\n",
        "\n",
        "\n",
        "use_cols = [\n",
        "    'x_position', 'y_position', 'speed', 'distance',\n",
        "    'direction', 'orientation', 'acceleration', 'sa'\n",
        "]\n",
        "train, feature_cols = create_features(train, tr_tracking, use_cols=use_cols)\n",
        "test, feature_cols = create_features(test, te_tracking, use_cols=use_cols)\n",
        "if torch.cuda.is_available():\n",
        "    train = train.to_pandas()\n",
        "    test = test.to_pandas()\n",
        "\n",
        "display(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "Hf3oEaNRz184",
        "outputId": "55a49d15-4fab-41f2-f75f-cb89b674cbb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-52cbd650f6d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;34m'direction'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'orientation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'acceleration'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sa'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m ]\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_tracking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_tracking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# training & inference\n",
        "# ==============================\n",
        "train_X = train[feature_cols]\n",
        "test_X = test[feature_cols]\n",
        "train_y = train['contact']\n",
        "cfg.folds = get_groupkfold(train, 'contact', 'game_play', cfg.num_fold)\n",
        "cfg.folds.to_csv(os.path.join(cfg.EXP_PREDS, 'folds.csv'), index=False)\n",
        "\n",
        "oof_pred = fit_xgboost(cfg, train_X, train_y, cfg.xgb_params, add_suffix=\"_xgb_1st\")\n",
        "sub_pred = pred_xgboost(test_X, cfg.EXP_MODEL, add_suffix=\"_xgb_1st\")"
      ],
      "metadata": {
        "id": "LRKUbFyDz8Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# optimize\n",
        "# ==============================\n",
        "def func(x_list):\n",
        "    score = matthews_corrcoef(train['contact'], oof_pred>x_list[0])\n",
        "    return -score\n",
        "\n",
        "x0 = [0.5]\n",
        "result = minimize(func, x0,  method=\"nelder-mead\")\n",
        "cfg.threshold = result.x[0]\n",
        "print(\"score:\", round(matthews_corrcoef(train['contact'], oof_pred>cfg.threshold), 5))\n",
        "print(\"threshold\", round(cfg.threshold, 5))\n",
        "\n",
        "test = add_contact_id(test)\n",
        "test['contact'] = (sub_pred > cfg.threshold).astype(int)\n",
        "test[['contact_id', 'contact']].to_csv('submission.csv', index=False)\n",
        "display(test[['contact_id', 'contact']].head())"
      ],
      "metadata": {
        "id": "a3Owg9Jc0Hn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bDAzXRv40Iyo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}